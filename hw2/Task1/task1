#!/usr/bin/env python2

import requests
from bs4 import BeautifulSoup
import time
import requests.packages.urllib3
import pdb
import sys	

def main():
  requests.packages.urllib3.disable_warnings()
  total_urls = open('task1_input.txt','r')
  result = drawgraph(total_urls)
  sys.stdout = open('task1_output.txt','w')
  for key,values in result.viewitems():
    print key.split('https://en.wikipedia.org/wiki/')[1],
    for value in values:
      print value.split('https://en.wikipedia.org/wiki/')[1],
    print "\n",
  sys.stdout.close()

def request(url):
  partial_links = []
  unique_links = []
  All_links = []
  try:
    request = requests.get(url)
    #pdb.set_trace()
    prefix_link = unicode('https://en.wikipedia.org')
    request_page = request.text
    soup=BeautifulSoup(request_page,'html.parser') 
    required_content = soup.find("div", dict(id="mw-content-text"))
    notrequired_content = ['thumb', 'navbox', 'reflist']
    for section in notrequired_content:
        for div in required_content.find_all('div',{'class': section}):
            div.decompose()
    for div in required_content.find_all('table',{'class': 'vertical-navbox'}):
        div.decompose()


    for link in required_content.find_all('a'):
        All_links.append(unicode(link.get('href')))
       
    for link in All_links:
      if  ("wiki" in link and "#" not in link and "index.php" not in link and ".org" not in link and ":" not in link and "wikimedia" not in link and "None" not in link and "/wiki/Main_Page" not in link and "www." not in link):   
          partial_links.append(prefix_link + link)
             

    for link in partial_links:
        if link not in unique_links:
          unique_links.append(link)
     
    return unique_links

  except:
    print "Issue with the connection"
    return unique_links
     


def drawgraph(total_urls):
  web_graph = {}
  crawled_links = []
  extract_urls = total_urls.readlines()
 # Remove new lines from url.
  for index in range(len(extract_urls)):
    extract_urls[index]=extract_urls[index].strip('\n')    
  for url in extract_urls:
    web_graph[url]=[] 
  for url in extract_urls:
    crawled_links = request(unicode(url))
    for link in crawled_links:
      if link in web_graph.viewkeys():
        if url not in web_graph[link]: 
          web_graph[link].append(url)

     
  return web_graph  
main()
