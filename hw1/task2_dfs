#!/usr/bin/env python2

import requests
from bs4 import BeautifulSoup
import time
import pdb
	

parent_links = []
crawled_links = []
child_links = []
depth = 5
current_depth = 1
tobe_added_child_links = []


def main():
  link = []
  print "Enter the url"
  url = raw_input() 
  print "Enter the keyword"
  keyword = raw_input()
  link.append(unicode(url))
  result = webcrawl(link,current_depth,keyword)
  file = open('Task2-dfs.txt','w')
  for link in result:
    file.write(link + "\n")
  file.close()
  

def request(url,current_depth,keyword):
  partial_links = []
  unique_links = []
  All_links = []
  try:
    request = requests.get(url)
    prefix_link = unicode('https://en.wikipedia.org')
    request_page = request.text
    soup=BeautifulSoup(request_page,'html.parser') 
    required_content = soup.find("div", dict(id="mw-content-text"))
    notrequired_content = ['thumb', 'navbox', 'reflist']
    for section in notrequired_content:
        for div in required_content.find_all('div',{'class': section}):
            div.decompose()
    for div in required_content.find_all('table',{'class': 'vertical-navbox'}):
        div.decompose()


    for link in required_content.find_all('a'):
        All_links.append(unicode(link.get('href')))
   
    if (keyword): 
      for link in All_links:
        if  ("#" not in link and "index.php" not in link and ".org" not in link and ":" not in link and "wikimedia" not in link and "None" not in link and "/wiki/Main_Page" not in link and "www." not in link and keyword in link.lower()):                
          partial_links.append(prefix_link + link)
   
    for link in partial_links:
        if link not in unique_links:
          unique_links.append(link)
    
    if url not in crawled_links:
      crawled_links.append(url)
      print url, ":" , current_depth, ":" , len(crawled_links)   

    return unique_links

  except:
    print "Issue with the connection"
    return unique_links



def webcrawl(link,current_depth,keyword):
  parent_links.extend(link) 
  child_links = request(parent_links.pop(0),current_depth,keyword)
  time.sleep(1)
  for index in range(len(child_links)):      
    if (len(crawled_links) < 1000 and current_depth < depth):
      pop_link = child_links.pop(0)      
      if pop_link not in crawled_links:      
        temp_link = []
        temp_link.append(pop_link)
        webcrawl(temp_link,current_depth + 1,keyword)
      else:
        continue
         
    else:
      return 
        
  return crawled_links      

main()
